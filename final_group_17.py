# -*- coding: utf-8 -*-
"""Final_GROUP_17.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AiZT2Q2tD_9ohedZLh85j1D4kbs2UaQ6
"""

from keras import preprocessing
from keras.layers import Embedding
from keras.models import Sequential
from keras.layers import Flatten, Dense
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np


# In[2]:


text = []
with open("X_data.txt", encoding="utf-8") as f:
    for review in f:
        review = review.lower()
        review = review.strip()
        text.append(review)

labels = []
with open("y_data.txt", encoding="utf-8") as f:
    for label in f:
        label = label.strip()
        label = int(label)
        labels.append(label)

# In[3]:


tokenizer = Tokenizer()
tokenizer.fit_on_texts(text)
sequences = tokenizer.texts_to_sequences(text)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

#Creating max_lenght for text
max_length = max([len(s.split()) for s in text])
print('Max lenght is %s ' % max_length)


# In[4]:


## Padding
data = pad_sequences(sequences, maxlen = max_length)
labels = np.asarray(labels)
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)


# In[5]:


max_words = len(word_index) + 1


# In[6]:


from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size= 0.2, random_state=1)

print(X_train.shape)
print(X_val.shape)
print(y_train.shape)
print(y_val.shape)

from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Dropout

##Define the model. 
model = Sequential()
model.add(Embedding(max_words, 100, input_length=max_length))
model.add(Conv1D(filters=64, kernel_size=8, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
print(model.summary())

model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])
history = model.fit(X_train, y_train,
                    epochs=2,
                    batch_size=100,
                    validation_data=(X_val, y_val))

test = []

with open("X_test.txt", encoding="utf-8") as f:
    for review in f:
        review = review.lower()
        review = review.strip()
        test.append(review)

sequences = tokenizer.texts_to_sequences(test)
x_test = pad_sequences(sequences, maxlen=201)

results = (model.predict(x_test) > 0.5).astype('int32')

answerlist = []
for i in results:
  if i == 1:
    answerlist.append('1' + '\n')
  else:
    answerlist.append('0' + '\n')

with open('answer.txt','w', newline='\n') as f:
    for line in answerlist:
        f.write(line)